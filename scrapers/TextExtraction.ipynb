{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import magic\n",
    "import os\n",
    "import pathlib\n",
    "import spacy\n",
    "from IPython.display import clear_output\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_file_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    with pdf_file_path.open('rb') as file:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        string_io = StringIO()\n",
    "        pdf_converter = TextConverter(resource_manager, string_io, codec='utf-8', laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, pdf_converter)\n",
    "        pages = PDFPage.get_pages(file, caching=True, check_extractable=False)\n",
    "\n",
    "        for i, page in enumerate(pages):\n",
    "            page_interpreter.process_page(page)\n",
    "            clear_output(wait=True)\n",
    "            print(\"Processed page\", i + 1, \"for\", pdf_file_path.name)\n",
    "        text = string_io.getvalue()\n",
    "        pdf_converter.close()\n",
    "        string_io.close()\n",
    "        return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def count_text(root_dir: Path):\n",
    "    count = 0\n",
    "    for file in root_dir.iterdir():\n",
    "        if file.is_dir():\n",
    "            count += count_text(file)\n",
    "        elif file.suffix == '.pdf':\n",
    "            count += 1\n",
    "        elif magic.from_file(file, mime=True).startswith(\"text/\"):\n",
    "            count += 1\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def process_file(file, progress):\n",
    "    if file.is_dir():\n",
    "        return search_for_text(file, progress)\n",
    "    elif file.suffix == '.pdf':\n",
    "        texts = extract_text_from_pdf(file)\n",
    "        progress.update(1)\n",
    "        return [texts]\n",
    "    elif magic.from_file(file, mime=True).startswith(\"text/\"):\n",
    "        with open(file, 'r') as f:\n",
    "            texts = f.read()\n",
    "            progress.update(1)\n",
    "            return [texts]\n",
    "    return []\n",
    "\n",
    "def search_for_text(root_dir: Path, progress: tqdm):\n",
    "    texts = []\n",
    "    for file in root_dir.iterdir():\n",
    "        texts = texts + process_file(file, progress)\n",
    "    # with concurrent.futures.ThreadPoolExecutor(max_workers=11) as executor:\n",
    "    #     futures = [executor.submit(process_file, file, progress) for file in root_dir.iterdir()]\n",
    "    #     for future in concurrent.futures.as_completed(futures):\n",
    "    #         texts = texts + future.result()\n",
    "    return texts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def delete_parsed_text_files(root_dir):\n",
    "    \"\"\"Deletes every file named parsed_text.txt in a directory and its subdirectories.\"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    for file_path in root_path.rglob('parsed_text.txt'):\n",
    "        file_path.unlink()\n",
    "# delete_parsed_text_files('data')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1514/1514 [5:16:35<00:00, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed page 1 for Doc3817220750 - Addendum 1.pdf\n"
     ]
    }
   ],
   "source": [
    "root = Path('data')\n",
    "total_files = count_text(root)\n",
    "pb = tqdm(total=total_files, smoothing=0)\n",
    "\n",
    "for folder in root.iterdir():\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "    parsed_text_file = folder.joinpath('parsed_text.txt')\n",
    "    if parsed_text_file.exists():\n",
    "        pb.update(count_text(folder))\n",
    "        continue\n",
    "\n",
    "    documents = search_for_text(folder, progress=pb)\n",
    "    with open(parsed_text_file, 'w') as f:\n",
    "        f.write(\"\\n\".join(documents))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 18/84 [14:44<54:02, 49.13s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [2], line 22\u001B[0m\n\u001B[1;32m     19\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(output_file, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     20\u001B[0m             f\u001B[38;5;241m.\u001B[39mwrite(sentences_text)\n\u001B[0;32m---> 22\u001B[0m \u001B[43mextract_sentences\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [2], line 14\u001B[0m, in \u001B[0;36mextract_sentences\u001B[0;34m(folder)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(parsed_text), chunk_size):\n\u001B[1;32m     13\u001B[0m     chunk \u001B[38;5;241m=\u001B[39m parsed_text[i:i \u001B[38;5;241m+\u001B[39m chunk_size]\n\u001B[0;32m---> 14\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     sentences\u001B[38;5;241m.\u001B[39mextend([sent\u001B[38;5;241m.\u001B[39mtext \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m doc\u001B[38;5;241m.\u001B[39msents])\n\u001B[1;32m     16\u001B[0m sentences_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(sentences)\n",
      "File \u001B[0;32m~/miniconda3/envs/ttb_scraper/lib/python3.10/site-packages/spacy/language.py:1011\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[0;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[1;32m   1009\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[1;32m   1010\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1011\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m   1012\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1013\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[1;32m   1014\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def extract_sentences(folder):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.max_length = 2000000\n",
    "    chunk_size = 100000\n",
    "    root_folder = pathlib.Path(folder)\n",
    "\n",
    "    total_files = len(list(root_folder.rglob(\"parsed_text.txt\")))\n",
    "    for file_path in tqdm(root_folder.rglob(\"parsed_text.txt\"), total=total_files):\n",
    "        with open(file_path, 'r') as f:\n",
    "            parsed_text = f.read()\n",
    "            sentences = []\n",
    "            for i in range(0, len(parsed_text), chunk_size):\n",
    "                chunk = parsed_text[i:i + chunk_size]\n",
    "                doc = nlp(chunk)\n",
    "                sentences.extend([sent.text for sent in doc.sents])\n",
    "            sentences_text = '\\n'.join(sentences)\n",
    "\n",
    "        output_file = file_path.parent / 'sentences.txt'\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(sentences_text)\n",
    "\n",
    "extract_sentences('data')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def search_for_identical_pdfs(root_dir):\n",
    "    total_deleted = 0\n",
    "    \"\"\"Searches for identical PDF files in a directory and its subdirectories.\"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    pdf_hashes = {}\n",
    "    for file_path in root_path.rglob('*.pdf'):\n",
    "        with file_path.open('rb') as file:\n",
    "            pdf_hash = hashlib.sha1(file.read()).hexdigest()\n",
    "            if pdf_hash not in pdf_hashes:\n",
    "                pdf_hashes[pdf_hash] = [file_path]\n",
    "            else:\n",
    "                pdf_hashes[pdf_hash].append(file_path)\n",
    "    for pdf_hash, pdf_files in pdf_hashes.items():\n",
    "        if len(pdf_files) > 1:\n",
    "            print(f\"PDF with hash {pdf_hash} occurs {len(pdf_files)} times:\")\n",
    "            for pdf_file in pdf_files:\n",
    "                print(f\"  {pdf_file}\")\n",
    "            directory_count = {}\n",
    "            for pdf_file in pdf_files:\n",
    "                directory = pdf_file.parts[0:2]\n",
    "                if directory in directory_count:\n",
    "                    directory_count[directory] += 1\n",
    "                else:\n",
    "                    directory_count[directory] = 1\n",
    "            for directory, count in directory_count.items():\n",
    "                if count > 1:\n",
    "                    for pdf_file in pdf_files:\n",
    "                        if pdf_file.parts[0:2] == directory:\n",
    "                            os.remove(pdf_file)\n",
    "                            total_deleted += 1\n",
    "                            print(f\"  Deleted {pdf_file}\")\n",
    "                            pdf_files.remove(pdf_file)\n",
    "                            break\n",
    "\n",
    "            print()\n",
    "    print(f'Total deleted: {total_deleted}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
