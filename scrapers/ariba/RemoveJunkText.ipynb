{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe11d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08597006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_junk(file: Path):\n",
    "    output_file = file.with_suffix('.sentences.txt')\n",
    "    if output_file.exists():\n",
    "        return\n",
    "    with open(file, 'r', errors='ignore') as f:\n",
    "        data = f.read()\n",
    "        if len(data) > nlp.max_length:\n",
    "            print(f'{file} is too long, using chunks...')\n",
    "            start = 0\n",
    "            while start < len(data):\n",
    "                chunk = data[start:start + nlp.max_length]\n",
    "                parsed = nlp(chunk)\n",
    "                with open(output_file, 'a') as f:\n",
    "                    for sentence in parsed.sents:\n",
    "                        f.write(\" \".join([f'{word}' for word in sentence if word.pos_ not in ['SPACE','PUNCT'] and word.dep_ not in ['punct','nummod']]) + '\\n')\n",
    "                start += nlp.max_length\n",
    "        else:\n",
    "            parsed = nlp(data)\n",
    "            with open(output_file, 'w') as f:\n",
    "                for sentence in parsed.sents:\n",
    "                    f.write(\" \".join([f'{word}' for word in sentence if word.pos_ not in ['SPACE','PUNCT'] and word.dep_ not in ['punct','nummod']]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0817470d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                                                                            | 0/1400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Doc3704154726/04_TT203001_Toronto_EIR_BlackCreekSanitaryTrunkSewer_9Mar21.parsed.txt is too long, using chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1400/1400 [04:06<00:00,  5.67it/s]\n"
     ]
    }
   ],
   "source": [
    "root = Path('data')\n",
    "file_list = list(root.rglob('*.parsed.txt'))\n",
    "file_list.sort(key=lambda f: os.path.getsize(f))\n",
    "file_count = len(file_list)\n",
    "\n",
    "for file in tqdm(file_list, total=file_count):\n",
    "    remove_junk(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf1354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
