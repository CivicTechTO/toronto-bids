{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6acd8d44-b195-441b-b602-ebeb9c7f40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.remote.errorhandler import NoSuchElementException\n",
    "from ariba_driver import Ariba\n",
    "from time import sleep\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from filemanage import extract_zip_and_move_html, move_pdfs, parse_html\n",
    "\n",
    "# System default download directory\n",
    "DOWNLOAD_DIRECTORY = Path.home() / 'Downloads'\n",
    "\n",
    "# Working directory\n",
    "REPO_DIRECTORY = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ce8802",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def wait_for_download(command, max_wait=1200) -> bool:\n",
    "    initial_length = len(list(DOWNLOAD_DIRECTORY.iterdir()))\n",
    "    command()\n",
    "    total_wait = 0\n",
    "    while len(list(DOWNLOAD_DIRECTORY.iterdir())) == initial_length:\n",
    "        sleep(15)\n",
    "        total_wait += 15\n",
    "        if total_wait > max_wait:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb63425d-c8f6-4d42-991a-a92a668b3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_directory_files(root: Path) -> int:\n",
    "    if not root.exists():\n",
    "        return 0\n",
    "    return len(list(root.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0965a8fb-9ad0-495c-9de6-545d7203897c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main_loop(has_clicked: bool = False) -> bool:\n",
    "    while not has_clicked: #We loop through RFPs until we find one we want to click on\n",
    "        elements = driver.find_elements(By.CLASS_NAME, 'ADTableBodyWhite') #Class name for open RFPs (and some closed ones!)\n",
    "        elements += driver.find_elements(By.CLASS_NAME, 'ADHiliteBlock') #Class name for closed RFPs (exclusively)\n",
    "        for element in elements:\n",
    "            try:\n",
    "                title = element.find_element(By.CLASS_NAME, 'QuoteSearchResultTitle') #Title is hyperlink\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "            title_text = title.text\n",
    "            if title_text in clicked:\n",
    "                continue\n",
    "            \n",
    "            print(f'{title.text}')\n",
    "\n",
    "            # Attempt to parse expiry date, so we can see if the RFP is open\n",
    "            date = element.find_elements(By.CLASS_NAME, 'paddingRight5')\n",
    "            if len(date) < 3:\n",
    "                request_expired = True\n",
    "                print('\\tNo date found')\n",
    "            else:\n",
    "                date = date[2].text\n",
    "                request_expired = dt.datetime.strptime(date[:-4], '%d %b %Y %I:%M %p') < dt.datetime.now()\n",
    "                print(f'\\tdate: {date}')\n",
    "\n",
    "            clicked.add(title_text)\n",
    "            title.click()\n",
    "\n",
    "            # Now we've moved from the listing page to the RFP page. First thing is to identify the doc number\n",
    "            document_id = driver.patiently_find_regex('(Doc\\d{10})')\n",
    "            print(f'\\tDocument id is {document_id}')\n",
    "\n",
    "            # Now we check if there are any PDFs to download on the listing page\n",
    "            noip = driver.find_elements(By.XPATH, '//a[contains(text(),\".pdf\")]')\n",
    "            for link in noip:\n",
    "                print(f'\\tPDF found, downloading {link.text}')\n",
    "                wait_for_download(lambda: link.click())\n",
    "\n",
    "            # Check to see if we've already downloaded the raw HTML, and the attachments\n",
    "            html_exists = Path(f'{REPO_DIRECTORY}/data/{document_id}.html').exists() or Path(\n",
    "                f'{REPO_DIRECTORY}/data/{document_id}/{document_id}.html'\n",
    "            ).exists()\n",
    "\n",
    "            # Zip might exist as a zip file, or as a directory - if it's the latter, we need to check that there's more than just the HTML file\n",
    "            zip_exists = Path(f'{REPO_DIRECTORY}/data/{document_id}.zip').exists() or count_directory_files(Path(\n",
    "                f'{REPO_DIRECTORY}/data/{document_id}'\n",
    "            )) > 1\n",
    "\n",
    "            # Print the results of our checks\n",
    "            print('\\tHTML exists' if html_exists else '\\tHTML does not exist')\n",
    "\n",
    "            if zip_exists:\n",
    "                print('\\tZip exists')\n",
    "            elif not request_expired:\n",
    "                print('\\tZip does not exist')\n",
    "            else:\n",
    "                print('\\tZip does not exist, but RFP is expired')\n",
    "\n",
    "            # If we haven't already downloaded the HTML, download it now\n",
    "            if not html_exists:\n",
    "                with open(f'{REPO_DIRECTORY}/data/{document_id}.html', 'w') as f:\n",
    "                    f.write(driver.page_source)\n",
    "            if (not zip_exists) and (not request_expired):\n",
    "                # If we don't have the attachments and the RFP is still open, download them\n",
    "                driver.patiently_click('//*[@id=\"_hfdr9c\"]')  #respond to posting\n",
    "                driver.patiently_click('//*[@id=\"_xjqay\"]')  #download content\n",
    "                driver.patiently_click('//*[@id=\"_hgesab\"]', wait_after=15)  #click download attachment\n",
    "                driver.patiently_click('//*[@id=\"_h_l$m\"]/span/div/label', wait_after=5)  #click select all\n",
    "                wait_for_download(\n",
    "                    lambda: driver.patiently_click('//*[@id=\"_5wq_j\"]')\n",
    "                )  #download attachments (for real)\n",
    "\n",
    "            # Now we're done with this RFP, so we go back to the listing page\n",
    "            driver.patiently_click('//*[@text()=\"Back to Search Results\"]', wait_after=5)\n",
    "            return True # True because we aren't finished\n",
    "        if not has_clicked:\n",
    "            # if we didn't find an RFP on this page, we should go to the next page.\n",
    "            # First, check if the next page button is clickable\n",
    "            next_button = driver.find_element(By.XPATH, '//*[@id=\"next\"]')\n",
    "            if next_button.get_attribute('class') == 'disabled':\n",
    "                # If it's not clickable, we're done\n",
    "                print('No more RFPs to click on')\n",
    "                return False # False because we are finished\n",
    "            else:\n",
    "                # If it is clickable, click it\n",
    "                driver.patiently_click('//*[@id=\"next\"]', wait_after=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54331c-f862-4151-8dee-c5e3389df324",
   "metadata": {},
   "outputs": [],
   "source": [
    "finished = False\n",
    "clicked = set()\n",
    "driver = Ariba(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "while not finished:\n",
    "    try:\n",
    "        finished = main_loop()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # Check if the issue is that we aren't logged in\n",
    "        if not driver.is_logged_in():\n",
    "            driver.login()\n",
    "        else:\n",
    "            driver.quit()\n",
    "            driver = Ariba(service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Move zips from download directory to repo's data directory\n",
    "for file in DOWNLOAD_DIRECTORY.iterdir():\n",
    "    if file.suffix == '.zip':\n",
    "        file.rename(REPO_DIRECTORY / 'data' / file.name)\n",
    "\n",
    "extract_zip_and_move_html(REPO_DIRECTORY / 'data')\n",
    "move_pdfs(DOWNLOAD_DIRECTORY, REPO_DIRECTORY / 'data')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parse_html(REPO_DIRECTORY / 'data').to_csv('metadata.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
